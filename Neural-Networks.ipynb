{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off by making a simple Neuron\\\\\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "**Building a Neuron: The Role of the Sigmoid Function, An Activation Function**\n",
    "\n",
    "Creating a neuron involves several steps and relies heavily on mathematics. Among these, the sigmoid function plays a crucial role.\n",
    "\n",
    "The sigmoid function, also known as the logistic function, introduces non-linearity into the neuron. This means it helps the model learn complex relationships between variables, unlike linear models where changes in one variable directly affect another (e.g., doubling the length of a rectangle doubles its width).\n",
    "\n",
    "**Understanding Non-Linearity:**\n",
    "\n",
    "In real-world scenarios, relationships between variables are often non-linear. For example, consider a car's speed (variable A) and its fuel consumption (variable B). Doubling the speed (A) wouldn't simply double the fuel consumption (B). This is because air resistance increases exponentially as speed increases (another variable C affecting B). This creates a roughly cubic relationship between speed and fuel consumption.\n",
    "\n",
    "Here's an illustration:\n",
    "\n",
    "* **Linear Assumption (incorrect):** One might assume that doubling a car's speed would simply double its fuel consumption.\n",
    "* **Reality (non-linear):** Doubling speed increases air resistance significantly, leading to a much higher (cubic) increase in fuel consumption.\n",
    "\n",
    "Data points:\n",
    "\n",
    "* At 30 mph, a car might use 24 mpg.\n",
    "* At 60 mph, instead of 12 mpg (linear assumption), it might use 30 mpg (non-linear effect).\n",
    "* At 90 mph, consumption might drop to 18 mpg (air resistance becomes more dominant).\n",
    "\n",
    "The sigmoid function allows neurons to capture these non-linear relationships, making them powerful tools for tackling complex problems.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "**Weights and Biases:**\n",
    "\n",
    "Often times we initialize the weights and biases as random. This is because we want to avoid having all neurons in a neural network be the same weight, as then it is likely that all neurons will behave the exact same and that is not helpful for learning different aspects of the data.\n",
    "\n",
    "**So what is a weight and a bias?**\n",
    "A weight is a variable within a neuron that serves to scale or de-scale a data point's importance.\n",
    "A bias is a constant variable added to data points whose whole purpose is to \"shift the activation function in a sense.\" This helps in handling inputs that are not centered around zero.\n",
    "\n",
    "**Evaluating a Predicted Data Point:**\n",
    "The evaluation of a data point is quite crucial, as it tells you how far you are from the prediction. There are two specific ways that we are going to cover in which you can evaluate a predicted data point.\n",
    "\n",
    "**Binary Cross Entropy Loss:**\n",
    "This is commonly used for binary classification tasks. It measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "**Formula:**\n",
    "$$\n",
    "-\\frac{1}{n}\\sum_{i=1}^{n}\\left[y_i \\log(y_i^{\\hat}) + (1-y_i) \\log(1-y_i^{\\hat})\\right]\n",
    "$$\n",
    "where \\(y_i^{\\hat}\\) is the predicted output and \\(y_i\\) is the actual output.\n",
    "\n",
    "**Mean Squared Error:**\n",
    "This one is used in regression tasks to measure the Euclidean distances (squared) between the predicted output and the actual output.\n",
    "**Formula:**\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}(y_i^{\\hat} - y_i)^2\n",
    "$$\n",
    "\n",
    "**Calculating Gradient:**\n",
    "Oh boy, this is really hard to explain but I'm gonna try my best. Basically, the gradient is a vector of partial derivatives of the loss function with respect to each parameter (weight and bias) in the network. It indicates the direction and magnitude of the steepest increase in the loss function. We use its negative to update parameters, moving towards minimizing the loss.\n",
    "\n",
    "**Steps:**\n",
    "1. Calculate the Error using either the loss function formula for binary cross-entropy loss or simply by calculating the difference between the predicted point and the actual point.\n",
    "   To compute the derivative of the loss with respect to the predicted output:\n",
    "   $$\n",
    "   y^{\\hat} - y\n",
    "   $$\n",
    "\n",
    "2. Calculate the Partial Derivatives:\n",
    "   1. Partial derivatives are kinda simple since you are calculating the derivative of a variable with respect to another variable, but you often treat variable b as a constant.\n",
    "      To compute the derivative of the predicted output with respect to the weighted sum:\n",
    "      $$\n",
    "      y^{\\hat}(1 - y^{\\hat})\n",
    "      $$\n",
    "\n",
    "      To compute the derivative of the weighted sum with respect to each weight:\n",
    "      $$\n",
    "      x_i\n",
    "      $$\n",
    "\n",
    "3. Use the Chain Rule:\n",
    "   $$\n",
    "   x_i \\cdot y^{\\hat}(1 - y^{\\hat}) \\cdot (y^{\\hat} - y)\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PseudoCode\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def initialize_parameters(input_size):\n",
    "    weights = np.random.rand(input_size)\n",
    "    bias = np.random.rand(1)\n",
    "    return weights, bias\n",
    "\n",
    "def forward_propagation(inputs, weights, bias):\n",
    "    weighted_sum = np.dot(inputs, weights) + bias\n",
    "    output = sigmoid(weighted_sum)\n",
    "    return output, weighted_sum\n",
    "\n",
    "def binary_cross_entropy_loss(predicted_output, actual_output):\n",
    "    epsilon = 1e-15\n",
    "    predicted_output = np.clip(predicted_output, epsilon, 1 - epsilon)\n",
    "    loss = - (actual_output * np.log(predicted_output) + (1 - actual_output) * np.log(1 - predicted_output))\n",
    "    return np.mean(loss)\n",
    "\n",
    "def calculate_gradients(inputs, predicted_output, actual_output, weighted_sum):\n",
    "    error = predicted_output - actual_output  # dL/dy\n",
    "    d_loss_d_predicted = error\n",
    "    d_predicted_d_weighted_sum = sigmoid_derivative(weighted_sum)  # dy/dz\n",
    "    d_weighted_sum_d_weights = inputs  # dz/dw\n",
    "    d_weighted_sum_d_bias = 1  # dz/db\n",
    "\n",
    "    gradients = d_loss_d_predicted * d_predicted_d_weighted_sum * d_weighted_sum_d_weights\n",
    "    bias_gradient = d_loss_d_predicted * d_predicted_d_weighted_sum * d_weighted_sum_d_bias\n",
    "\n",
    "    return gradients, bias_gradient\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
